[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset-ignore folder which you will have to create manually. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Stage and commit the files just like you would any other file.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour clean_data.R file in the scripts folder is the file where you will import the raw data that you download, clean it, and write .rds file(s) (using write_rds) that you’ll load in your analysis page. If desirable, you can have multiple scripts that produce different derived data sets, just make sure to link to them on this page.\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\). Instead, use the here function from the here package to avoid path problems.\n\n\nClean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which will usually be .rds files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\n\n\nLink to the original data source: https://catalog.data.gov/dataset/nypd-arrest-data-year-to-date The dataset can be found from data.gov and we used the csv file under the link above.\n\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nThe data includes all arrests made by the NYPD in New York City, manually extracted quarterly and reviewed by the Office of Management Analysis and Planning. Originally created in 2018 and last updated in January 2025, it provides detailed information on crime type, location, time of enforcement, and suspect demographics. This publicly available dataset offers insights into the patterns and nature of police enforcement activity.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.\n\n\nOut of the 19 variables in the dataset, we focused on selecting only the variables relevant to our analysis - race category, gender, age group, borough of arrest, offense category, and police description of the crime.\n\nUse figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncleaned_data &lt;- read_rds(\"dataset/cleaned_dataset.rds\")\ncleaned_data %&gt;%\n  count(perp_race, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(perp_race, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Number of Arrests by Race\",\n    x = \"Race\",\n    y = \"Number of Arrests\"\n  )\n\n\n\n\n\n\n\n\nThe bar chart shows the number of arrests by race category. The majority of arrests are of individuals identified as Black. Other categories including White Hispanic and Black Hispanic appear frequently, whereas some categories — including American Indian/Alaskan Native — are far less common. This plot summarizes which groups are most represented in the dataset and also raises questions about potential systemic factors contributing to arrest trends.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained.\n\n\nData cleaning: Link to cleaned data file: https://github.com/sussmanbu/ma4615-sp25-final-project-team-6/blob/main/dataset/cleaned_dataset.rds\nTo clean the NYPD arrest dataset, we focused on selecting only the variables relevant to our analysis - race category, gender, age group, borough of arrest, offense category, and police description of the crime. We removed fully empty columns and unnecessary categories such as arrest date, arrest key, and jurisdiction code. We standardized column names and cleaned string variables by trimming whitespace and applying consistent formatting (e.g., title-casing offense descriptions). All categorical variables, such as age group, race, and borough were converted to factors for easier future analysis. We also eliminated any rows with missing values for easier future analysis as well. Here is the code we used to clean our dataset:\nlibrary(tidyverse) library(janitor) library(lubridate) raw_data &lt;- read_csv(“dataset/NYPD_Arrest_Data_Year_to_Date.csv”) cleaned_data &lt;- raw_data %&gt;% clean_names() %&gt;%\nselect(where(~ !all(is.na(.)))) %&gt;%\nselect(\nperp_race,\nperp_sex,\nage_group,\narrest_boro,\nofns_desc,\npd_desc\n) %&gt;% mutate( perp_race = factor(str_trim(perp_race)),\nperp_sex = factor(str_trim(perp_sex)), age_group = factor(str_trim(age_group)), arrest_boro = factor(str_trim(arrest_boro)), ofns_desc = str_to_title(str_trim(ofns_desc)),\npd_desc = str_to_title(str_trim(pd_desc)) ) %&gt;% drop_na()\nwrite_rds(cleaned_data, “dataset/cleaned_dataset.rds”)"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show long quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\nThis comes from the index.qmd file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 7\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 6\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nTeam 6\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 5\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2025\n\n\nTeam 6\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 4\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nTeam 6\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nTeam 6\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\nTeam 6\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Tips\n\n\n\n\n\nSome small but important tips to follow. \n\n\n\n\n\nOct 4, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2025-04-25-blog-post-7/blog-post-7.html",
    "href": "posts/2025-04-25-blog-post-7/blog-post-7.html",
    "title": "Blog Post 7",
    "section": "",
    "text": "The interactive will be built using Shiny in R and will include clear, guided visuals like line charts and possibly a map view for location-based insights. I’m currently in the process of cleaning the dataset and identifying the most useful variables for filtering and analysis. The interactive aspect—letting users control what data they see—will help highlight interesting patterns and make the experience more personal and impactful.\nProgress\nWe’ve explored the NYPD arrest + ACS data and cleaned it into combined_data. That object has borough names, offense description, demographics, coordinates, and unemployment rate—but no date column—so for our “big picture” we’ll start with a static bar chart of arrests by borough, then let the user drill in via filters.\nInteractive We’re building the Arrest Trends Explorer Shiny dashboard to let users explore NYPD arrest data from multiple angles: - A dynamic time-series of total arrests, with the ability to switch between line charts, bar charts, and map overlays. - Filters for borough, offense type, and demographic slices (race, sex, age group).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(shiny)\n\ncombined_data &lt;- readRDS(\"dataset/combined_nypd_acs_with_coords_fixed.rds\") %&gt;%\n  mutate(unemployment_rate = as.numeric(unemployment_rate))\n\nboro_choices   &lt;- c(\"All\", sort(unique(combined_data$borough)))\noffense_choices &lt;- c(\"All\", sort(unique(combined_data$ofns_desc)))\nrace_choices   &lt;- c(\"All\", sort(unique(combined_data$perp_race)))\nsex_choices    &lt;- c(\"All\", sort(unique(combined_data$perp_sex)))\nage_choices    &lt;- sort(unique(combined_data$age_group))\n\nInteractive Here’s the Shiny app code for our Arrest Trends Explorer. The main plot is a bar chart of arrests by borough. The sidebar inputs let you filter further by offense or demographic slice.\n\nui &lt;- fluidPage(\n  titlePanel(\"Arrest Trends Explorer\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"boro\",    \"Borough:\",           choices = boro_choices),\n      selectInput(\"offense\", \"Offense Type:\",      choices = offense_choices),\n      selectInput(\"demo\",    \"Demographic Filter:\", choices = c(\"None\",\"Race\",\"Sex\",\"Age Group\")),\n\n      conditionalPanel(\n        \"input.demo == 'Race'\",\n        selectInput(\"race\", \" Race:\", choices = race_choices)\n      ),\n      conditionalPanel(\n        \"input.demo == 'Sex'\",\n        selectInput(\"sex\", \" Sex:\", choices = sex_choices)\n      ),\n      conditionalPanel(\n        \"input.demo == 'Age Group'\",\n        selectInput(\"age\", \" Age Group:\", choices = age_choices)\n      )\n    ),\n\n    mainPanel(\n      plotOutput(\"barPlot\", height = \"500px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  filtered &lt;- reactive({\n    df &lt;- combined_data\n\n    if (input$boro    != \"All\") df &lt;- df %&gt;% filter(borough   == input$boro)\n    if (input$offense != \"All\") df &lt;- df %&gt;% filter(ofns_desc == input$offense)\n\n    if (input$demo == \"Race\"      && input$race != \"All\")\n      df &lt;- df %&gt;% filter(perp_race == input$race)\n\n    if (input$demo == \"Sex\"       && input$sex  != \"All\")\n      df &lt;- df %&gt;% filter(perp_sex  == input$sex)\n\n    if (input$demo == \"Age Group\" && input$age  != \"\")\n      df &lt;- df %&gt;% filter(age_group == input$age)\n\n    df\n  })\n\n  output$barPlot &lt;- renderPlot({\n    filtered() %&gt;%\n      count(borough) %&gt;%\n      ggplot(aes(x = reorder(borough, n), y = n, fill = borough)) +\n      geom_col(show.legend = FALSE) +\n      coord_flip() +\n      labs(\n        title = \"Number of Arrests by Borough\",\n        x     = NULL,\n        y     = \"Arrest Count\"\n      ) +\n      theme_minimal()\n  })\n}\n\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents\n\n\nSteps Left The interactive dashboard will let users explore NYPD arrest data from different angles. - The goal is to make the data accessible and engaging by allowing users to view big picture trends like total arrests over time. - Users will also be able to “zoom in” on more specific patterns. - Users will be able to filter the data by borough, offense type, and demographic categories such as age or race. - This gives them the power to investigate questions that are personally meaningful or socially relevant, such as whether certain neighborhoods experience more arrests for specific types of crimes, or how trends shift over the course of a year."
  },
  {
    "objectID": "posts/2025-03-05-blog-post-2/blog-post-2.html",
    "href": "posts/2025-03-05-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Our dataset focused on New York Police Department arrest Data and was separated according to what kind of crime was committed, along with the location where it occurred. Members of the Office of Management Analysis review sets of data over time and update the database. NYC Open Data acquires more cases from agencies that are responsible for managing such cases. NYPD serves as one of the oldest police departments and is notorious for dealing with heinous crimes, some of them being setting a man on fire. The data we found was referenced from the data.gov website. The data that we viewed was from the original source, as it is the data set that police agencies from NYC Open Data view and update.\nThe dataset looks unorganized, with the age groups of the felonies being in random order and race category in random order, along with the dates not being in chronological order. The sample population would be the first 10,000 columns, as any further columns have empty datasets. The dataset isn’t necessarily biased, as it is a mere record of criminal activity not in a specific order or specialization. This data is used by journalists, researchers and even advocacy groups who all analyze practices of crime and arrests within New York City. There has been other research on it from numerous policy research and academic papers that use the data to study racial disparities in several areas. This data is not specifically used for policy decisions, however it can be involved in discussions regarding racial profiling and police reform, enabling a change in the criminal justice system in New York City. Some common questions that others asked about this data are as follows:\n1.Are certain racial or ethnic groups disproportionately arrested?\n2.How do arrest patterns vary by borough or precinct?\n3.How has NYPD enforcement shifted over time?\n4.What types of crimes result in the most arrests?"
  },
  {
    "objectID": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "href": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "Datasets we will be combining are: Primary Dataset - NYPD Arrest Data (2024). Our cleaned dataset includes variables such as perp_race, perp_sex, age_group, ofns_desc (offense description), and arrest_boro (borough where the arrest occurred) throughout the year 2024. Secondary Dataset - American Community Survey (ACS) 2023 1-Year Estimates – We will be honing in on the boroughs that are included in our primary dataset - Bronx, Staten Island, Brooklyn, Manhattan, and Queens. Specifically, we will be using the following metrics for each of the 5 boroughs - Unemployment Rate (Table: S2301) - Median Household Income (Table: S1901) - Poverty Rate (Table: S1701)\nHow the datasets will be combined: Both datasets contain borough identifiers, so we will combine the datasets using the arrest_boro field from our cleaned NYPD Arrest data and matching borough names in the ACS data (Bronx, Staten Island, Brooklyn, Manhattan, Queens).\nWe chose to combine the arrest data with economic indicators from the 2023 American Community Survey (ACS) 1-Year Estimates for a few reasons: - The NYPD dataset provides information on who is being arrested and where, but it doesn’t explain why certain boroughs might have more arrests and crimes. Hence, including borough-level data on unemployment, poverty, and income for the respective boroughs adds broader context that can be used to answer our overarching research question. - Arrest or crime patterns are greatly influenced by structural conditions in an area, such as economic inequality and unemployment. Including economic data will enable us to examine whether boroughs with higher poverty, higher unemployment and lower household income experience higher arrest rates. - Furthermore, the 2023 ACS 1-Year Estimates are the most recent reliable economic data that is available and align with the 2024 NYPD arrest data.\nInitial findings: From our cleaned NYPD dataset, we found that Brooklyn had the highest number of arrests (72,325 arrests in 2024), followed by Manhattan, the Bronx, and Queens. Staten Island had the fewest number of arrests. This pattern aligns with economic indicators from the ACS 2023 dataset Brooklyn has relatively high unemployment (6.7%) and moderate poverty (18.3%), suggesting that these economic conditions could be contributing to higher arrests. With the lowest poverty and unemployment rates, Staten Island has by far the lowest arrest count (11,055). Hence, the conclusion is that boroughs with lower income, higher unemployment, and higher poverty tend to have more arrests in general.\nPotential difficulties in combining the datasets: The NYPD arrest data is based on the year 2024, whereas the most recent economic data available from the ACS is 2023. Hence, some comparisons may not reflect the exact state of the economic conditions during the time of arrest. Secondly, some of the offense descriptions in the NYPD arrest data are vague, which can make it difficult to categorize offenses as economically motivated or not."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2025-02-24-blog-post-1/blog-post-1.html",
    "href": "posts/2025-02-24-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "NYPD Arrest Data (Year to Date):\n\nhttps://catalog.data.gov/dataset/nypd-arrest-data-year-to-date\na.260,504 rows & 19 columns\nb.The dataset includes all arrests made by the NYPD in New York City, manually extracted quarterly and reviewed by the Office of Management Analysis and Planning. Originally created in 2018 and last updated in January 2025, it provides detailed information on crime type, location, time of enforcement, and suspect demographics. This publicly available dataset offers insights into the patterns and nature of police enforcement activity.\nc.Yes, dataset can be loaded and cleaned.\nd.1.How do arrest rates differ across the five boroughs in NYC based on 2025 data? 2. Is there a correlation between the time of arrest and the type of crime committed? 3. How do arrest rates by race vary across different crime types?\ne.The potential challenges include missing or incomplete data (e.g. some arrests lack key details) and variations in how certain crimes are recorded, resulting in unclear interpretations.\n\nDrug overdose death rates, by drug type, sex, age, race, and Hispanic origin: United States:\n\nhttps://catalog.data.gov/dataset/drug-overdose-death-rates-by-drug-type-sex-age-race-and-hispanic-origin-united-states-3f72f\na.6229 rows and 15 columns\nb.This dataset is compiled by the National Center for Health Statistics (NCHS) through the National Vital Statistics System (NVSS), which gathers mortality data from state-issued death certificates. Its main purpose is to track drug overdose deaths by various demographics—such as drug type, sex, age, race, and Hispanic origin—so that public health officials and policymakers can monitor trends, identify high-risk groups, and allocate resources effectively.\nc.Yes, able to clean the dataset.\nd.How have drug overdose death rates changed over time in the United States?\ne.A key challenge is dealing with missing or inconsistent data entries across different states, which can complicate cleaning efforts and affect the accuracy of trend analysis.\n\nFertility Rate By Race\n\nhttps://www.kaggle.com/datasets/loveall/fertility-rate-by-race\na.5 columns, 423 rows\nb.No references labeled on where it was originally collected. Most likely collected from the NCHS (National Center for Health Statistics). This data is also most likely collected from sources from the United States.\nc.Yes, able to clean the dataset, however the data is already very cleaned, there are a few missing values.\nd.Which race has the highest fertility rate? How does time correlate with fertility rate for each respective race?\ne.There are not a lot of conclusions to extrapolate, such as why these values are the way they are, which is a much more interesting and valuable insight to find."
  },
  {
    "objectID": "posts/2024-10-04-general-tips/general-tips.html",
    "href": "posts/2024-10-04-general-tips/general-tips.html",
    "title": "General Tips",
    "section": "",
    "text": "Use the tidyverse!\nYou don’t have to tell me what kind of chart something is. For example, the below is not a useful start to a sentence.\n\n\nThe graph presents a horizontal bar chart …\n\n\nEach page should be largely standalone.\nSometimes small tables or even inline numbers are better than a figure.\nRedundant colors (e.g. bar charts where each bar is a different color that doesn’t signify anything) often don’t help.\nProvide some details on how much data was removed in your cleaning process.\nUse the tidyverse!\nImagine I’m an impatient boss. Show me only what is important and relevant.\nCleaning must be entirely in R\nDon’t say things like, well if only everyone did like so and so than everything would be better. There are many things hiding behind the data that would go to explain things. This is an example of a bad conclusion.\n\n\nThe world could benefit form modeling its education systems after Europe’s.\n\nIt is fine to talk about how the European system is better according to certain metrics, but don’t assume that can easily translate to other regions.\n\nDon’t talk about your “journey”. The blog posts tell the story of your journey. The main pages should focus on the data and your findings.\n\n\nUse the tidyverse!\nNo but seriously, when asking ChatGPT to do your project for you, make sure to tell it to use the tidyverse, not base R."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "title": "Blog Post 6",
    "section": "",
    "text": "Final Steps As our team approaches the final stages of the project, we’ve assessed our progress to identify areas of the project where more work is needed. We’ve successfully conducted an indepth analysis and produced several charts/graphs highlighting demographic differences in violent crime victimization in Chicago. Our visuals include clear breakdowns by sex and race, and support a story about inequalities in crime victimization. We’ve also formulated a preliminary narrative thesis highlighting the significant disparities based on sex and race to form the foundation of the Big Picture article.\nThe first task we still need to complete is the interactive dashboard component. We have a template and dataset ready, but we still need to finalize our Shiny dashboard to make sure it supports our narrative by allowing users to explore different demographics interactively. Also, we’re editing our Big Picture article’s to create an engaging, clear, and informative style similar to popular media. Next steps include finalizing a creative and engaging headline, finishing the explanatory text around each visualization, and making sure the interactive components flow with the narrative.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\ncombined_data &lt;- readRDS(\"dataset/combined_nypd_acs_with_coords_fixed.rds\")\nnyc_map &lt;- st_read(\"dataset/nybb.shp\")\n\nReading layer `nybb' from data source \n  `/Users/martin/Downloads/BU/SPRING 2025/MA 415/Team 6/dataset/nybb.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 913175.1 ymin: 120128.4 xmax: 1067383 ymax: 272844.3\nProjected CRS: NAD83 / New York Long Island (ftUS)\n\n\nArrest Count by Borough\n\ncombined_data %&gt;%\n  count(arrest_boro) %&gt;%\n  ggplot(aes(x = reorder(arrest_boro, -n), y = n, fill = arrest_boro)) +\n  geom_col() +\n  labs(title = \"Total Arrests by Borough in 2024\", x = \"Borough\", y = \"Number of Arrests\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nArrests vs. Unemployment Rate\n\ncombined_data %&gt;%\n  count(arrest_boro) %&gt;%\n  left_join(distinct(combined_data, arrest_boro, unemployment_rate), by = \"arrest_boro\") %&gt;%\n  ggplot(aes(x = unemployment_rate, y = n, label = arrest_boro)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  geom_text(vjust = -1) +\n  labs(title = \"Arrests vs Unemployment Rate by Borough\",\n       x = \"Unemployment Rate (%)\", y = \"Number of Arrests\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nArrests by Race and Sex\n\ncombined_data %&gt;%\n  count(arrest_boro, perp_race) %&gt;%\n  ggplot(aes(x = arrest_boro, y = n, fill = perp_race)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Arrests by Borough and Race\", x = \"Borough\", y = \"Number of Arrests\", fill = \"Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\ncombined_data %&gt;%\n  count(arrest_boro, perp_sex) %&gt;%\n  ggplot(aes(x = arrest_boro, y = n, fill = perp_sex)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Arrests by Borough and Sex\", x = \"Borough\", y = \"Number of Arrests\", fill = \"Sex\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop Offense Types in High vs Low Unemployment Areas\n\ncombined_data %&gt;%\n  distinct(arrest_boro, unemployment_rate) %&gt;%\n  mutate(unemployment_level = ifelse(unemployment_rate &gt;= median(unemployment_rate, na.rm = TRUE),\n                                     \"High Unemployment\", \"Low Unemployment\")) -&gt; boro_unemp_group\n\ncombined_data %&gt;%\n  left_join(boro_unemp_group, by = \"arrest_boro\") %&gt;%\n  count(unemployment_level, ofns_desc) %&gt;%\n  group_by(unemployment_level) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  slice_max(prop, n = 10, with_ties = FALSE) %&gt;%\n  ggplot(aes(x = reorder(ofns_desc, prop), y = prop, fill = unemployment_level)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(title = \"Top Offense Types in High vs Low Unemployment Boroughs\",\n       x = \"Offense Description\", y = \"Proportion\", fill = \"Unemployment Level\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nArrest Map - Colored by Race\n\narrest_points &lt;- combined_data %&gt;%\n  filter(!is.na(longitude), !is.na(latitude)) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326, remove = FALSE)\n\nset.seed(42)\nplot_data &lt;- arrest_points %&gt;% sample_n(3000)\n\nggplot() +\n  geom_sf(data = nyc_map, fill = \"gray95\", color = \"black\") +\n  geom_sf(data = plot_data, aes(color = perp_race), size = 1, alpha = 0.6) +\n  labs(title = \"Arrest Locations in NYC (2024)\",\n       subtitle = \"Each point represents an arrest. Colored by race.\",\n       color = \"Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\n\ncombined_data &lt;- readRDS(\"dataset/combined_nypd_acs_with_coords_fixed.rds\")\nnyc_map &lt;- st_read(\"dataset/nybb.shp\")\n\nReading layer `nybb' from data source \n  `/Users/martin/Downloads/BU/SPRING 2025/MA 415/Team 6/dataset/nybb.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 913175.1 ymin: 120128.4 xmax: 1067383 ymax: 272844.3\nProjected CRS: NAD83 / New York Long Island (ftUS)\n\ncombined_data &lt;- combined_data %&gt;%\n  mutate(unemployment_rate = as.numeric(unemployment_rate))\n\nborough_unemp &lt;- combined_data %&gt;%\n  group_by(borough) %&gt;%\n  summarise(unemployment_rate = mean(unemployment_rate, na.rm = TRUE))\n\nnyc_map &lt;- nyc_map %&gt;%\n  mutate(borough = case_when(\n    BoroName == \"Manhattan\" ~ \"MANHATTAN\",\n    BoroName == \"Brooklyn\" ~ \"BROOKLYN\",\n    BoroName == \"Queens\" ~ \"QUEENS\",\n    BoroName == \"Bronx\" ~ \"BRONX\",\n    BoroName == \"Staten Island\" ~ \"STATEN ISLAND\"\n  )) %&gt;%\n  left_join(borough_unemp %&gt;% select(borough, unemployment_rate), by = \"borough\") %&gt;%\n  select(-matches(\"^unemployment_rate\\\\..*\"))\n\narrest_points &lt;- combined_data %&gt;%\n  filter(!is.na(longitude), !is.na(latitude)) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326, remove = FALSE)\n\nset.seed(42)\nplot_data &lt;- arrest_points %&gt;% sample_n(3000)\n\nggplot() +\n  geom_sf(data = nyc_map, aes(fill = unemployment_rate), color = \"white\") +\n  scale_fill_gradient(low = \"lightyellow\", high = \"darkred\", na.value = \"grey80\") +\n  geom_sf(data = plot_data, aes(color = perp_race), size = 1, alpha = 0.6) +\n  labs(\n    title = \"NYC Arrest Locations Overlaid on Borough Unemployment Rates (2024)\",\n    subtitle = \"Boroughs shaded by unemployment rate. Points show arrests colored by race.\",\n    fill = \"Unemployment Rate (%)\",\n    color = \"Race\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2025-03-31-blog-post-4/blog-post-4.html",
    "href": "posts/2025-03-31-blog-post-4/blog-post-4.html",
    "title": "Blog Post 4",
    "section": "",
    "text": "In our previous posts 2 and 3, we identified some of the big, obvious trends in the NYPD Arrest dataset.\nFor Blog Post 4, we took a two-pronged approach:\n1.Breadth: We examined how these trends relate to other variables in the dataset.\n2.Depth: We zoomed in on one specific relationship to understand its underlying drivers, and we began our initial exploration into statistical modeling.\nEDA Output 1: Offense Type by Race\nWe started by analyzing the relationship between offense type and race. By grouping the data by race and then identifying the top five offenses for each racial group, we created a summary table that highlights the relative frequency of various offenses.\nKey insights from this analysis included:\nEven though Felony Assault and Petit Larceny appear frequently across all races, the ranking and counts differ.\nSome racial groups show a different offense mix, suggesting that while the overall trend of high counts in common offenses is evident, there are underlying variations that merit further exploration.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\ncleaned_data &lt;- read_rds(\"dataset/cleaned_dataset.rds\")\nrace_offense_table &lt;- cleaned_data %&gt;%\n  count(perp_race, ofns_desc, sort = TRUE) %&gt;%\n  group_by(perp_race) %&gt;%\n  slice_max(n, n = 5) %&gt;%\n  ungroup() %&gt;%\n  arrange(perp_race, desc(n))\nggplot(race_offense_table, aes(x = reorder(ofns_desc, n), y = n, fill = perp_race)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ perp_race, scales = \"free_y\") +\n  coord_flip() +\n  labs(\n    title = \"Top 5 Offense Types by Race\",\n    x = \"Offense Type\",\n    y = \"Number of Arrests\"\n  )\n\n\n\n\n\n\n\n\nEDA Output 2: Top Offenses by Gender\nNext, we compared offense frequencies by gender. A bar chart of the top three offenses for males and females revealed that both genders share similar offense categories. However, males are arrested in significantly higher numbers across these categories.\nKey insights here are:\nThe types of offenses remain consistent between genders, but the volume of arrests varies greatly.\nThis observation raises further questions about whether demographic distribution or other confounding factors are influencing the observed differences.\n\ncleaned_data &lt;- read_rds(\"dataset/cleaned_dataset.rds\")\ncleaned_data %&gt;%\n  count(perp_sex, ofns_desc, sort = TRUE) %&gt;%\n  group_by(perp_sex) %&gt;%\n  slice_max(n, n = 3) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = reorder(ofns_desc, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ perp_sex, scales = \"free_y\") +\n  labs(\n    title = \"Top 3 Offense Categories according to Gender\",\n    x = \"Offense Category\",\n    y = \"Number of Arrests\"\n  )\n\n\n\n\n\n\n\n\nModeling Approach:\nResponse Variable: Arrest Borough\nPredictor Variables: Offense Description, Race, and Age Group\nData Transformations: We consolidated infrequent offense types through factor lumping and transformed categorical variables into dummy variables to make them compatible with the model.\nModel Type: Since our response variable is categorical, we used multinomial logistic regression.\nOur next steps include:\nExperimenting with more advanced models (like random forests or gradient boosting) to capture non-linear interactions.\nTesting interaction terms (such as race by offense or age by race) in our logistic regression.\nIncorporating additional variables, like temporal data, to assess how these patterns evolve over time.\nPotentially normalizing by borough population to better understand enforcement intensity.\nBy moving from descriptive analysis to predictive modeling, we’re beginning to answer not only “what is happening?” but also “why is it happening?” This progression is essential as it deepens our insights and opens up new avenues for both academic inquiry and policy discussion."
  },
  {
    "objectID": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "href": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "To clean the NYPD arrest dataset, we focused on selecting only the variables relevant to our analysis - race category, gender, age group, borough of arrest, offense category, and police description of the crime. We removed fully empty columns and unnecessary categories such as arrest date, arrest key, and jurisdiction code. We standardized column names and cleaned string variables by trimming whitespace and applying consistent formatting (e.g., title-casing offense descriptions). All categorical variables, such as age group, race, and borough were converted to factors for easier future analysis. We also eliminated any rows with missing values for easier future analysis as well. Here is the code for our cleaned dataset which can be found under the folder dataset:\nlibrary(tidyverse) library(janitor) library(lubridate) raw_data &lt;- read_csv(“dataset/NYPD_Arrest_Data_Year_to_Date.csv”) cleaned_data &lt;- raw_data %&gt;% clean_names() %&gt;%\nselect(where(~ !all(is.na(.)))) %&gt;%\nselect(\nperp_race,\nperp_sex,\nage_group,\narrest_boro,\nofns_desc,\npd_desc\n) %&gt;% mutate( perp_race = factor(str_trim(perp_race)),\nperp_sex = factor(str_trim(perp_sex)), age_group = factor(str_trim(age_group)), arrest_boro = factor(str_trim(arrest_boro)), ofns_desc = str_to_title(str_trim(ofns_desc)),\npd_desc = str_to_title(str_trim(pd_desc)) ) %&gt;% drop_na()\nwrite_rds(cleaned_data, “dataset/cleaned_dataset.rds”)\nData Analysis:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(lubridate)\ncleaned_data &lt;- read_rds(\"dataset/cleaned_dataset.rds\")\nrace_offense_table &lt;- cleaned_data %&gt;%\n  count(perp_race, ofns_desc, sort = TRUE) %&gt;%\n  group_by(perp_race) %&gt;%\n  slice_max(n, n = 5) %&gt;%\n  arrange(perp_race, desc(n))\n\nprint(race_offense_table)\n\n# A tibble: 35 × 3\n# Groups:   perp_race [7]\n   perp_race                      ofns_desc                        n\n   &lt;fct&gt;                          &lt;chr&gt;                        &lt;int&gt;\n 1 AMERICAN INDIAN/ALASKAN NATIVE Assault 3 & Related Offenses   199\n 2 AMERICAN INDIAN/ALASKAN NATIVE Felony Assault                  78\n 3 AMERICAN INDIAN/ALASKAN NATIVE Petit Larceny                   78\n 4 AMERICAN INDIAN/ALASKAN NATIVE Vehicle And Traffic Laws        58\n 5 AMERICAN INDIAN/ALASKAN NATIVE Miscellaneous Penal Law         50\n 6 ASIAN / PACIFIC ISLANDER       Assault 3 & Related Offenses  2722\n 7 ASIAN / PACIFIC ISLANDER       Felony Assault                1617\n 8 ASIAN / PACIFIC ISLANDER       Petit Larceny                 1258\n 9 ASIAN / PACIFIC ISLANDER       Miscellaneous Penal Law       1239\n10 ASIAN / PACIFIC ISLANDER       Dangerous Drugs                902\n# ℹ 25 more rows\n\n\nTo better understand the relationship between race and type of offense, we created a table displaying the top five offenses by race. This summarized table allows us to identify whether certain offenses are disproportionately represented in certain racial groups and enables us to discover any unusual or unexpected patterns in the dataset.\n\ncleaned_data %&gt;%\n  count(perp_sex, ofns_desc, sort = TRUE) %&gt;%\n  group_by(perp_sex) %&gt;%\n  slice_max(n, n = 3) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = reorder(ofns_desc, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ perp_sex, scales = \"free_y\") +\n  labs(\n    title = \"Top 3 Offense Categories according to Gender\",\n    x = \"Offense Category\",\n    y = \"Number of Arrests\"\n  )\n\n\n\n\n\n\n\n\nThe bar charts display the 3 most common offense categories for males and females in the dataset. While both genders share similar top offenses such as, assault-related charges and petit larceny, males are arrested at a higher frequency for each of the 3 offense categories. This visualization allows for an easier comparison of actual counts, pointing to the disparities in number of offenses respective to gender.\n\ncleaned_data %&gt;%\n  count(perp_race, sort = TRUE)\n\n# A tibble: 7 × 2\n  perp_race                           n\n  &lt;fct&gt;                           &lt;int&gt;\n1 BLACK                          122049\n2 WHITE HISPANIC                  69131\n3 BLACK HISPANIC                  26549\n4 WHITE                           26161\n5 ASIAN / PACIFIC ISLANDER        14838\n6 UNKNOWN                           956\n7 AMERICAN INDIAN/ALASKAN NATIVE    819\n\n\nThis table looks at the distribution of arrests by race categories. We found that a disproportionate number of arrests (122049) were recorded for individuals who are ‘Black’, which is way higher than other races. In comparison, categories including American Indian/Alaskan Native and Unknown had fewer than 1,000 arrests each. These patterns could be showing real-world differences, but also allows us to raise further questions when analyzing our dataset."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below.\nhttps://github.com/sussmanbu/ma4615-sp25-final-project-team-6\nFares Sakaan\nhttps://github.com/faressakaan\nAlexis Park\nRhoshnne Paramasivam\nhttps://github.com/RhoshnneP\nTaimur Ahmad\nhttps://github.com/taiahmad\nMinghao Yang\nhttps://github.com/martinyang11\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]